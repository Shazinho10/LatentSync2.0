# Enhanced-LatentSync: Improving Lip-Sync with a Dedicated MEL U-Net and FiLM

This project is an extension of the work presented in the paper "LatentSync: Taming Audio-Conditioned Latent Diffusion Models for Lip Sync with SyncNet Supervision" (arXiv:2412.09262v2). My goal is to further improve the accuracy and naturalness of the lip-synchronization by introducing novel architectural modifications to the audio-conditioning pipeline.

### Overview
The original LatentSync model is a state-of-the-art, end-to-end framework for generating high-resolution, lifelike talking videos. It leverages a latent diffusion model (LDM) conditioned on audio features. The authors identified a "shortcut learning problem" where the model tends to rely on visual cues rather than the audio input. Their solution involves strong SyncNet supervision and a careful masking strategy to force the model to learn the correlation between audio and lip movements.

This implementation builds upon that strong foundation and explores new methods for processing and injecting the audio signal, aiming to provide the diffusion model with even richer and more context-aware audio features.

### Suggestions for Improvement
I recommend two major architectural changes to improve the model's use of audio signals for more precise lip synchronization.

1. **Specialized MEL U-Net for Audio Feature Enhancement:**  
   Introduce a secondary U-Net, termed the MEL U-Net, specifically designed to process audio embeddings. Rather than feeding audio features from Whisper directly into the primary model, these features would first be refined by this dedicated U-Net. This approach is expected to help the model better capture the complex temporal structures in speech — ranging from brief phonetic cues to longer prosodic patterns — before leveraging them to drive the visual output.

2. **Hybrid Conditioning Using FiLM and Cross-Attention:**  
   Modify the audio conditioning mechanism in the main visual U-Net by combining Feature-wise Linear Modulation (FiLM) with cross-attention. The original design uses only cross-attention. A hybrid method would enable the model to simultaneously determine *what* visual features to produce (via cross-attention) and *how* to produce them (via FiLM), which could result in more nuanced and accurate lip movement generation.

These recommendations are grounded in thorough literature research:  
- **Feature-wise Linear Modulation:** [https://arxiv.org/pdf/1709.07871](https://arxiv.org/pdf/1709.07871)  
- **TryonDiffusion:** [https://tryondiffusion.github.io/](https://tryondiffusion.github.io/)


### MEL U-Net Implementation Details:

The MEL U-Net is designed as a classic U-Net architecture but adapted for sequential, one-dimensional data (the audio embeddings).

Input: It takes the sequence of audio embeddings generated by the Whisper encoder as its input.

### Architecture: 

Each block in the encoder and decoder paths of the MEL U-Net consists of a residual block for feature extraction, followed by a self-attention block.

### Purpose of Self-Attention: 

The self-attention mechanism is crucial here, as it allows the MEL U-Net to weigh the importance of different audio segments relative to each other. This helps in modeling the long-range temporal dependencies in speech, which is vital for natural-sounding lip movements.

### Output: 

The network outputs a refined sequence of audio embeddings of the same dimension and length as the input, which now contains richer contextual information.

### Architectural Flow:

Here is a high-level overview of the modified architecture incorporating both proposals:

**Audio Encoding:** A raw audio waveform is converted into a melspectrogram and then fed into the pre-trained Whisper Encoder to produce a sequence of audio embeddings.

**Audio Feature Refinement (MEL U-Net):** This sequence of embeddings is then passed through my new MEL U-Net. This network processes the temporal sequence to produce a refined set of audio feature vectors.

**Visual Diffusion with Hybrid Conditioning:** The refined audio features are injected into the main Visual U-Net using two parallel mechanisms within each block:

**Cross-Attention:** As in the original model, to align audio and visual features.

**FiLM:** To generate scale (
gamma) and shift (
beta) parameters that modulate the visual feature maps.

**Decoding:** The final denoised latent is passed through the VAE Decoder to produce the output video frame with synchronized lips.

### Diagram
Following is the high level diagram of the implementation where a Mel Unet has been introduced which caters to the audio embeddings only with self-attention each of its block.


![alt text](latentsync_2.jpg)